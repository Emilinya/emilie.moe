<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>optimization</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="ss=style.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#motivation">Motivation</a>
<ul>
<li><a href="#what-is-a-parameter">What is a parameter?</a></li>
<li><a href="#why-do-we-want-to-optimize-parameters">Why do we want to
optimize parameters?</a></li>
<li><a href="#mortivation">Mortivation</a></li>
</ul></li>
<li><a href="#numerical-optimization">Numerical optimization</a>
<ul>
<li><a href="#newtons-method">Newton’s method</a></li>
<li><a href="#global-newton">Global Newton</a></li>
<li><a href="#gradient-descent">Gradient descent</a></li>
<li><a href="#backtracking-line-search">Backtracking line
search</a></li>
<li><a href="#improved-backtracking-line-search">Improved backtracking
line search</a></li>
<li><a href="#a-combined-approach">A combined approach</a></li>
<li><a href="#local-vs-global-minima">Local vs global minima</a></li>
</ul></li>
<li><a href="#parameter-optimization">Parameter optimization</a>
<ul>
<li><a href="#the-error-function">The error function</a></li>
<li><a href="#differentiating-the-error-function">Differentiating the
error function</a></li>
<li><a href="#uncertainty-estimation">Uncertainty estimation</a>
<ul>
<li><a href="#assumptions">Assumptions</a></li>
<li><a href="#covariance">Covariance</a></li>
</ul></li>
</ul></li>
<li><a href="#omega-optimizer">Omega Optimizer</a>
<ul>
<li><a href="#normal-distribution">Normal distribution</a></li>
<li><a href="#sine-wave">Sine wave</a></li>
<li><a href="#mortivated">Mortivated</a></li>
<li><a href="#conclusion-and-further-developments">Conclusion and
further developments</a></li>
</ul></li>
<li><a href="#how-it-is-actually-done">How it is actually done</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>I have long been interested in how to find the optimal parameters to
model experimental data. I have created many projects with the goal of
optimizing parameters automatically, and with my latest version, aptly
named <em>Omega Optimizer</em>, I feel like I finally have something
that works. In this blog post I want to explain what parameter
optimization is, how you can do it numerically, and how my <em>Omega
Optimizer</em> works.</p>
<h2 id="motivation">Motivation</h2>
<h3 id="what-is-a-parameter">What is a parameter?</h3>
<p>If we are optimizing parameters, a good question is, what <em>is</em>
a parameter? Let us look at the function <span
class="math inline">\(f(x) = ax + b\)</span>. This is the equation for a
line with intercept <span class="math inline">\(a\)</span> and slope
<span class="math inline">\(b\)</span>. Here, <span
class="math inline">\(x\)</span> is the variable, and <span
class="math inline">\(a\)</span> and <span
class="math inline">\(b\)</span> are parameters. We can think about this
as a parameterized function, where we typically write <span
class="math inline">\(f(x; a, b)\)</span> to indicate that we also want
to vary the parameters.</p>
<h3 id="why-do-we-want-to-optimize-parameters">Why do we want to
optimize parameters?</h3>
<p>Many times when doing experimental physics, we know which function
describes what we are measuring, but want to find its parameters. For
instance, Ohm’s law says <span class="math inline">\(U = RI\)</span>, so
if we want to find the resistance of a resistor, we can measure the
voltage over the resistor as we vary the current. We then have the
function <span class="math display">\[
  U(I; R) = RI,
\]</span> and want to find <span class="math inline">\(R\)</span>. This
is called linear regression, and it is quite easy; there are formulas
that give the optimal slope and intercept for any linear regression
problem. In fact, using linear algebra we can find the exact optimal
parameters for any function of the form <span class="math display">\[
    f(x; a_1, a_2, \ldots, a_n) = a_1g_1(x) + a_2g_2(x) + \cdots +
a_ng_n(x),
\]</span> where <span class="math inline">\(a_1, a_2, \ldots,
a_n\)</span> are parameters and <span class="math inline">\(g_1, g_2,
\ldots, g_n\)</span> are functions that only depend on <span
class="math inline">\(x\)</span>.</p>
<p>The problem is then non-linear regression. What if your parameters
are in another function, like <span class="math inline">\(f(t; \omega) =
\sin(\omega t)\)</span>? In this particular case you can use a Fourier
transform to find <span class="math inline">\(\omega\)</span>, and in
general there are often approaches that give you the parameters, but
they are heavily dependent on the specific problem you are working with.
To create a general non-linear parameter optimizer, we must instead turn
to numerical methods.</p>
<h3 id="mortivation">Mortivation</h3>
<p>In this blog post I want to use a real-world example to show how a
parameter optimizer can be useful. One day my brother sent me this
image:</p>
<p><img src="media/mortimg.jpeg" /></p>
<p>along with the question “What function is this, and how do I
linearize it?”. These two questions are linked, if I can find a function
<span class="math inline">\(f(x)\)</span> that describes the curve, the
inverse <span class="math inline">\(f^{-1}(x)\)</span> will linearize
the data. The question is then, what function is it? It acts a bit like
<span class="math inline">\(\frac{x}{x+1}\)</span>. Specifically, it
looked like <span class="math inline">\(\frac{x^3}{x^3+1}\)</span>:</p>
<p><img src="media/curve.png" /></p>
<p>I therefore hypothesized that the data could be described by the
function <span class="math display">\[f(x; a, b, c, n) =
\frac{ax^n}{bx^n + 1} + c.\]</span></p>
<p>This is a non-linear regression problem! When I solved this problem
for my brother initially, I used my then newly developed parameter
optimizer, but it could not find the optimal parameters, so I found them
manually instead. This was a big part of my motivation for this new
parameter optimizer, and I am happy to report that <em>Omega
Optimizer</em> managed to find better parameters than I found
manually.</p>
<h2 id="numerical-optimization">Numerical optimization</h2>
<h3 id="newtons-method">Newton’s method</h3>
<p>Parameter optimization, unsurprisingly, involves optimizing some
function, that is, we need to find its minimum. So, how do you minimize
a function numerically? One of the most popular optimization algorithms
is Newton’s method, which is more commonly known in its root-finding
form. Newton’s method finds the solution <span
class="math inline">\(x_{\text{root}}\)</span> to the equation <span
class="math inline">\(f(x_{\text{root}}) = 0\)</span> using the sequence
<span class="math display">\[x_{n+1} = x_n -
\frac{f(x_n)}{f&#39;(x_n)},\]</span> starting at some initial value
<span class="math inline">\(x_0\)</span>. We know that this converges to
<span class="math inline">\(x_{\text{root}}\)</span> in the limit as
<span class="math inline">\(n\rightarrow\infty\)</span>, but
numerically, we typically don’t want our program to run forever. We must
therefore also add some stopping condition, which is typically if <span
class="math display">\[|x_{n+1} - x_{n}| =
\left|\frac{f(x_n)}{f&#39;(x_n)}\right| &lt; \epsilon,\]</span> where
<span class="math inline">\(\epsilon\)</span> is some small value within
numerical accuracy.</p>
<p>To convert this method into a minimization algorithm, we can use the
fact that the minimum <span class="math inline">\(x_{\min}\)</span> of
some function <span class="math inline">\(f\)</span> satisfies <span
class="math inline">\(f&#39;(x_{\min}) = 0\)</span>, that is, minimizing
a function is the same as finding the roots of its derivative. By
substituting <span class="math inline">\(f&#39;\)</span> for <span
class="math inline">\(f\)</span> in Newton’s method, we get its
minimization form: <span class="math display">\[x_{n+1} = x_n -
\frac{f&#39;(x_n)}{f&#39;&#39;(x_n)}.\]</span> Here we could again stop
if the step is sufficiently small, but I will instead stop if <span
class="math inline">\(|f&#39;(x_n)| &lt; \epsilon\)</span>, as that is
the stopping criterion for the other method I will use as well.</p>
<p>One important thing to note is that <span
class="math inline">\(f&#39;(x) = 0\)</span> does not imply that <span
class="math inline">\(x\)</span> is a minimum; it can also be a maximum.
To ensure that we converge to a minimum we need <span
class="math inline">\(f&#39;&#39;(x_0) &gt; 0\)</span>, that is, the
function must be locally convex.</p>
<p>Newton’s method also works for multivariable functions. If we
substitute the gradient for the derivative and the hessian for the
double derivative, we get the sequence <span
class="math display">\[\mathbf{x}_{n+1} = \mathbf{x}_n -
(\mathbf{H}f(\mathbf{x}_n))^{-1}\nabla f(\mathbf{x}_n),\]</span> which
converges to <span class="math inline">\(\mathbf{x}_{\min}\)</span> from
some initial point <span class="math inline">\(\mathbf{x}_0\)</span>,
stopping once <span class="math inline">\(||\nabla f(\mathbf{x}_n)||
&lt; \epsilon\)</span>. To recap, if you have a multivariable function
<span class="math inline">\(f(x, y)\)</span>, the gradient is <span
class="math display">\[
\def\arraystretch{1.4}
\nabla f(x, y) = \begin{bmatrix} \frac{\partial f}{\partial x} \\
\frac{\partial f}{\partial y}\end{bmatrix}
\]</span> and the hessian is <span class="math display">\[
\def\arraystretch{1.6}
Hf(x, y) = \begin{bmatrix} \frac{\partial ^2f}{\partial x^2} &amp;
\frac{\partial ^2f}{\partial x \partial y}\\ \frac{\partial
f^2}{\partial y \partial x} &amp; \frac{\partial f^2}{\partial y^2}
\end{bmatrix}.
\]</span> This obviously extends to more variables.</p>
<p>To ensure Newton’s method reaches a minimum, we now need the hessian
to be positive-definite at <span class="math inline">\(x_0\)</span>,
which is just the multivariable extension of <span
class="math inline">\(f&#39;&#39;(x_0) &gt; 0\)</span>.</p>
<h3 id="global-newton">Global Newton</h3>
<p>One problem with Newton’s method is that it can be unstable. For some
functions, there are initial values which makes the iteration diverge.
One such function is <span class="math inline">\(f(x) =
\arctan(x)\)</span>, where any initial point <span
class="math inline">\(|x_0| \gtrsim 1.5\)</span> causes divergence. The
problem is that arctan is a very flat function for large values of <span
class="math inline">\(x\)</span>, so <span
class="math inline">\(f&#39;\)</span> is very small, and the Newton step
<span class="math inline">\(\frac{f}{f&#39;}\)</span> becomes very
large, overshooting the root at <span
class="math inline">\(x=0\)</span>. Starting at <span
class="math inline">\(x_0=1.5\)</span>, we get <span
class="math inline">\(\frac{f}{f&#39;} \approx -3.19\)</span>, so <span
class="math inline">\(x_1 = -1.69\)</span>. This leads to an even larger
step of 4.02, so the next value is <span class="math inline">\(x_2 =
2.32\)</span>, and so on. The problem is caused by the steps being too
large, so an idea for a fix is to simply decrease the step sizes by
multiplying with some factor <span class="math inline">\(0 &lt; \gamma
&lt; 1\)</span>, called a damping factor. This works, a factor is 0.5 is
enough for Newton’s method to converge in this case, but it still
diverges for <span class="math inline">\(|x_0| \gtrsim 3\)</span>. To
ensure convergence, we need <span class="math inline">\(\gamma \approx
0\)</span>, but then the method would become unreasonably slow.</p>
<p>For a smarter approach, we can use the fact that a set of points
<span class="math inline">\(\{x_0,x_1,\ldots,x_n\}\)</span> converges to
a root of <span class="math inline">\(f\)</span> if <span
class="math inline">\(|f(x_0)| &gt; |f(x_1)| &gt; \cdots &gt;
|f(x_n)|.\)</span> That is, the method will converge to a root if every
iteration decreases the absolute value of the function. What we need to
do then is, for each iteration, see if the function value deceased, and
if not, multiply with a damping factor small enough such that the
function does decrease. This ensures that most steps are not dampened,
so we keep Newton’s speed, but by ensuring that each step decreases the
function, we also guarantee that the iteration will converge. This
approach is called the global Newton’s method.</p>
<p>The global Newton’s method is aimed at finding roots, but we want to
minimize a function. The equivalent of ensuring <span
class="math inline">\(|f|\)</span> decreases is then ensuring <span
class="math inline">\(||\nabla f||\)</span> decreases, but we can
instead use <span class="math inline">\(f\)</span> as the condition.
This ensures that each iteration decreases the function value, and
therefore that the iteration converges to a minimum. Allowing <span
class="math inline">\(||\nabla f||\)</span> to increase restricts
Newton’s method less, which I have found to speed up convergence in some
cases. The global Newton’s method for minimization can be described
algorithmically as</p>
<blockquote>
<p>Set <span class="math inline">\(\mathbf{x}=\mathbf{x}_0\)</span>, for
some initial value <span
class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p><strong>while</strong> <span class="math inline">\(||\nabla
f(\mathbf{x})|| &gt; \epsilon\)</span> <strong>do</strong></p>
<blockquote>
<p>Solve <span class="math inline">\(\mathbf{H}f(\mathbf{x})\Delta x =
\nabla f(\mathbf{x})\)</span> for <span class="math inline">\(\Delta
x\)</span>.</p>
<p>Set <span class="math inline">\(f_{\text{prev}} = f(x)\)</span> and
<span class="math inline">\(\gamma = 1\)</span>.</p>
<p><strong>while</strong> <span class="math inline">\(f(x - \gamma\Delta
x) &gt; f_{\text{prev}}\)</span> <strong>do</strong></p>
<blockquote>
<p>Set <span class="math inline">\(\gamma =
\frac{\gamma}{2}\)</span></p>
</blockquote>
<p><strong>end while</strong></p>
<p>Set <span class="math inline">\(x = x - \gamma\Delta x\)</span></p>
</blockquote>
<p><strong>end while</strong></p>
<p>Return <span class="math inline">\(\mathbf{x}\)</span></p>
</blockquote>
<h3 id="gradient-descent">Gradient descent</h3>
<p>The global Newton’s method works well in most cases, but it has a
major flaw; it does not work if the hessian is not positive-definite. To
avoid this problem, we need a method that does not use the hessian, such
as gradient descent. If you have ever had a vector calculus course, you
probably know that the gradient of a function points in the direction of
steepest ascent. Conversely, the negative of the gradient points in the
direction of steepest descent. We can use this to find the minimum of a
function; if we calculate the gradient at a point, we can follow the
negative of the gradient to find a new point, which will have a lower
function value. Repeat this enough times, and you will reach the
minimum.</p>
<p>Gradient descent works using the sequence <span
class="math display">\[\mathbf{x}_{n+1} = \mathbf{x}_n - t\nabla
f(\mathbf{x}_n),\]</span> where the value <span
class="math inline">\(t\)</span> is the size of the step you take along
the gradient at each point. This value is very important; if it is too
small, the iteration will converge very slowly, but if it is too large,
the iteration will diverge. You might notice that this is very similar
to Newton’s method, and indeed, another way of motivating gradient
descent is to say we are approximating the inverse of the hessian with
some positive scalar <span class="math inline">\(t\)</span>.</p>
<h3 id="backtracking-line-search">Backtracking line search</h3>
<p>As mentioned, the step size <span class="math inline">\(t\)</span> in
gradient descent is very important. It can be left as a constant value,
but this often leads to slow convergence. Instead, you can find an
optimal value at each iteration by finding the step size <span
class="math inline">\(t_n\)</span> which minimizes <span
class="math inline">\(f(\mathbf{x}_n - t\nabla f(\mathbf{x}_n))\)</span>
with respect to <span class="math inline">\(t\)</span>. This is called a
line search as you are minimizing a linear cross-section instead of the
full function. Finding the exact solution to the line search equation is
both costly and often unnecessary, so many methods for quickly finding
approximate solutions have been developed. One such method is called
backtracking line search, which takes in an initial step size <span
class="math inline">\(t_0\)</span> and two control parameters <span
class="math inline">\(\tau\in(0, 1)\)</span> and <span
class="math inline">\(c\in(0, 1)\)</span>, and then performs the
following steps:</p>
<blockquote>
<p>Set <span class="math inline">\(m = c||\nabla
f(\mathbf{x})||^2\)</span> and <span
class="math inline">\(t=t_0\)</span>.</p>
<p><strong>while</strong> <span class="math inline">\(f(\mathbf{x}) −
f(\mathbf{x} - t\nabla f(\mathbf{x})) &lt; tm\)</span>
<strong>do</strong></p>
<blockquote>
<p>Update <span class="math inline">\(t = \tau t\)</span></p>
</blockquote>
<p><strong>end while</strong></p>
<p>Return <span class="math inline">\(t\)</span></p>
</blockquote>
<p>This algorithm starts with a large initial step size, and decreases
it until it is small enough to give a meaningful improvement. This
results in a relatively large step size, so the descent will converge
quickly, but the step size is still small enough to avoid the iteration
diverging.</p>
<h3 id="improved-backtracking-line-search">Improved backtracking line
search</h3>
<p>While playing around with backtracking line search, I found two
problems. The first is the initial step size <span
class="math inline">\(t_0\)</span>. You typically don’t know a good
maximum for the step size a priori, so to ensure the algorithm can find
a good step size, you must choose a very large <span
class="math inline">\(t_0\)</span>. This feeds into the second problem,
which is that the optimal step size typically does not change a lot
between descent steps, but backtracking line search starts at the same
<span class="math inline">\(t_0\)</span> each time, which might result
in many unnecessary calls to <span class="math inline">\(f\)</span>. A
simple fix to this would be to set <span
class="math inline">\(t_0\)</span> to the previous optimal value, but if
you do this, <span class="math inline">\(t_0\)</span> can only stay the
same size or shrink, which typically results in <span
class="math inline">\(t_0\)</span> converging to 0. To avoid this
problem, I have created a modified backtracking line search algorithm,
which is based on the idea that you typically want <span
class="math inline">\(t\)</span> to be as large as possible. The
modified algorithm is:</p>
<blockquote>
<p>Set <span class="math inline">\(m = c||\nabla
f(\mathbf{x})||^2\)</span> and <span
class="math inline">\(t=t_0\)</span>, where <span
class="math inline">\(t_0\)</span> is the previous best <span
class="math inline">\(t\)</span> if this is not the first iteration, and
<span class="math inline">\(1\)</span> if it is.</p>
<p>Define <span class="math inline">\(\text{accept}(t) := (f(\mathbf{x})
− f(\mathbf{x} - t\nabla f(\mathbf{x})) \geq tm).\)</span></p>
<p>Set <code>increased_t</code> to false.</p>
<p><strong>while</strong> <span
class="math inline">\(\text{accept}(t)\)</span> <strong>do</strong></p>
<blockquote>
<p>Update <span class="math inline">\(t = t / \tau\)</span></p>
<p>Set <code>increased_t</code> to true.</p>
</blockquote>
<p><strong>end while</strong></p>
<p>Update <span class="math inline">\(t = \tau t\)</span>.</p>
<p><strong>if</strong> <code>increased_t</code> is false
<strong>then</strong></p>
<blockquote>
<p><strong>while</strong> not <span
class="math inline">\(\text{accept}(t)\)</span> <strong>do</strong></p>
<blockquote>
<p>Update <span class="math inline">\(t = \tau t\)</span></p>
</blockquote>
<p><strong>end while</strong></p>
</blockquote>
<p><strong>end if</strong></p>
<p>Return <span class="math inline">\(t\)</span></p>
</blockquote>
<p>This lets <span class="math inline">\(t\)</span> increase as long as
the acceptance criterion is fulfilled, which makes the algorithm more
flexible. From my experimentation, this seems like a direct improvement
to backtracking line search, so I am probably not the first person to
come up with the idea.</p>
<h3 id="a-combined-approach">A combined approach</h3>
<p>I have now described two optimization algorithms with complimentary
benefits and drawbacks. Gradient descent is a robust algorithm, it will
typically converge to a minimum even for a bad initial value, but it
converges to the minimum relatively slowly. On the other hand, the
global Newton’s method converges very quickly if the hessian at the
initial value is positive-definite, but if it is not, it will not work.
The logical thing to do is then to use both methods, where you do
gradient descent steps while the hessian is negative, and then switch to
Newton to quickly converge to the minimum. This leads to the following
algorithm:</p>
<blockquote>
<p>Set <span class="math inline">\(x_{best} = x_0\)</span>.</p>
<p><strong>for</strong> <span
class="math inline">\(N=10^1,10^2,\ldots,10^4\)</span>
<strong>do</strong></p>
<blockquote>
<p>Do gradient descent with <span class="math inline">\(N\)</span> steps
and initial value <span class="math inline">\(x_{best}\)</span> to get
result <span class="math inline">\(x_{GD}\)</span>.</p>
<p><strong>if</strong> gradient descent converged
<strong>do</strong></p>
<blockquote>
<p>Return <span class="math inline">\(x_{GD}\)</span>.</p>
</blockquote>
<p><strong>end if</strong></p>
<p>Do Newton’s method with 10 steps and initial value <span
class="math inline">\(x_{GD}\)</span> to get result <span
class="math inline">\(x_{NM}\)</span>.</p>
<p><strong>if</strong> <span class="math inline">\(f(x_{NM}) &lt;
f(x_{GD})\)</span> <strong>do</strong></p>
<blockquote>
<p><strong>if</strong> Newton’s method converged <strong>do</strong></p>
<blockquote>
<p>Return <span class="math inline">\(x_{NM}\)</span>.</p>
</blockquote>
<p><strong>else</strong></p>
<blockquote>
<p>Do Newton again with <span class="math inline">\(10^4\)</span> steps,
return result.</p>
</blockquote>
<p><strong>end if</strong></p>
</blockquote>
<p><strong>end if</strong></p>
<p>Set <span class="math inline">\(x_{best}= x_{GD}\)</span>.</p>
</blockquote>
<p><strong>end for</strong></p>
<p>Return <span class="math inline">\(x_{best}\)</span></p>
</blockquote>
<h3 id="local-vs-global-minima">Local vs global minima</h3>
<p>For now, I have assumed that the function we are minimizing has only
one minimum, so if the optimization methods converge, they give a good
result. Unfortunately, this is most often not the case. We want to reach
the minimum with the lowest value, called the global minimum, but there
are also many non-optimal minima, called local minima. There are many
ways of avoiding local minima, but the most reliable method is starting
at an initial set of parameters that are reasonably close to the global
minimum. Luckily, for parameter optimization, it is quite easy to tell
visually how good a set of parameters are by plotting the resulting
function together with the data points. This means a simple gui that
lets you vary parameters and visually see how the function changes is
good enough to avoid local minima, although it is a manual approach.</p>
<h2 id="parameter-optimization">Parameter optimization</h2>
<h3 id="the-error-function">The error function</h3>
<p>I have now written a lot about general multivariable functions, but
how will this help us with parameter optimization? Which function do we
want to find the minimum of?</p>
<p>Let’s say we have experimentally measured some data. We have measured
a set of results <span class="math inline">\(\{f_i\}\)</span> at
positions <span class="math inline">\(\{x_i\}\)</span>. We know that the
data should conform to some function <span class="math inline">\(f(x;
\theta_1, \theta_2, \ldots, \theta_n).\)</span> We define the parameter
vector <span class="math inline">\(\boldsymbol{\theta} = (\theta_1,
\theta_2, \ldots, \theta_n),\)</span> and want to find the parameter
vector <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>
such that <span class="math inline">\(f(x_i;
\hat{\boldsymbol{\theta}})\)</span> most closely matches <span
class="math inline">\(f_i\)</span> for each <span
class="math inline">\(i\)</span>. More specifically, we want to minimize
the average of the distances <span class="math inline">\(|f_i - f(x_i;\
\boldsymbol{\theta})|\)</span> for every <span
class="math inline">\(i\)</span>. If we have <span
class="math inline">\(N\)</span> data points, we can define the mean
difference as <span class="math display">\[
e(\boldsymbol{\theta}) = \frac{1}{N}\sum\limits_{i=1}^N |f_i - f(x_i;\
\boldsymbol{\theta})|.
\]</span> This is almost the function we want to minimize, but there is
one problem; we need to differentiate the error function, but the
absolute value function does not have a continuous derivative. We
therefore want to minimize the mean squared differences instead, giving
us <span class="math display">\[
   E(\boldsymbol{\theta}) = \frac{1}{N}\sum\limits_{i=1}^N (f_i -
f(x_i;\ \boldsymbol{\theta}))^2.
\]</span> Note that although this is not the same function as <span
class="math inline">\(e(\boldsymbol{\theta})\)</span>, any minima of
<span class="math inline">\(e(\boldsymbol{\theta})\)</span> will also be
minima of <span
class="math inline">\(E(\boldsymbol{\theta})\)</span>.</p>
<h3 id="differentiating-the-error-function">Differentiating the error
function</h3>
<p>To use gradient descent and Newton’s method, we need to find both
<span class="math inline">\(\nabla E\)</span> and <span
class="math inline">\(\mathbf{H} E\)</span>. We start with the
gradient:</p>
<p><span class="math display">\[
    \begin{align*}
        \frac{\partial E}{\partial \theta_k} &amp;=
\frac{\partial}{\partial \theta_k}
        \left(\frac{1}{N}\sum\limits_{i=1}^N (f_i - f(x_i;\
\boldsymbol{\theta}))^2\right) \\
        &amp;= \frac{1}{N}\sum\limits_{i=1}^N \frac{\partial}{\partial
\theta_k}(f_i - f(x_i;\ \boldsymbol{\theta}))^2 \\
        &amp;= -\frac{2}{N}\sum\limits_{i=1}^N (f_i - f(x_i;\
\boldsymbol{\theta}))\frac{\partial f}{\partial \theta_k} \\
        \implies \nabla E(\boldsymbol{\theta}) &amp;=
-\frac{2}{N}\sum\limits_{i=1}^N (f_i - f(x_i;\
\boldsymbol{\theta}))\nabla f(x_i;\ \boldsymbol{\theta}) \\
    \end{align*}
\]</span></p>
<p>We can then find the hessian: <span class="math display">\[
    \begin{align*}
        \frac{\partial E}{\partial \theta_l \partial \theta_k} &amp;=
\frac{\partial }{\partial \theta_l}\left(-\frac{2}{N}\sum\limits_{i=1}^N
(f_i - f(x_i;\ \boldsymbol{\theta}))\frac{\partial f}{\partial
\theta_k}\right) \\
        &amp;= -\frac{2}{N}\sum\limits_{i=1}^N \frac{\partial}{\partial
\theta_l}\left((f_i - f(x_i;\ \boldsymbol{\theta}))\frac{\partial
f}{\partial \theta_k}\right) \\
        &amp;= -\frac{2}{N}\sum\limits_{i=1}^N -\frac{\partial
f}{\partial \theta_l}\frac{\partial f}{\partial \theta_k} + (f_i -
f(x_i;\ \boldsymbol{\theta}))\frac{\partial f}{\partial \theta_l
\partial \theta_k} \\
        \implies \mathbf{H} E(\boldsymbol{\theta}) &amp;=
\frac{2}{N}\sum\limits_{i=1}^N \nabla f(\nabla f)^T - (f_i - f(x_i;\
\boldsymbol{\theta}))\mathbf{H}f(x_i;\ \boldsymbol{\theta}) \\
    \end{align*}
\]</span></p>
<h3 id="uncertainty-estimation">Uncertainty estimation</h3>
<p>We now have everything we need to create a program that calculates
optimal parameters, but there is still something we need to add. Going
back to the example of estimating resistance using Ohm’s law, our
program would currently just give some value, like “<span
class="math inline">\(3.1\ \Omega\)</span>”. This is fine in a casual
context, but for the result to be truly useful, we need to estimate the
uncertainty of the measurement. A standard estimate for the uncertainty
is the standard deviation, and we write a measurement of a resistance
<span class="math inline">\(\hat{R}\)</span> with corresponding standard
deviation <span class="math inline">\(\Delta R\)</span> as <span
class="math inline">\(\hat{R} \pm \Delta R\)</span>, which tells us that
there is a 68 % probability that the true resistance <span
class="math inline">\(R^*\)</span> is in the interval <span
class="math inline">\([\hat{R} - \Delta R, \hat{R} + \Delta
R].\)</span></p>
<p>For a practical example, let us say we are doing quality control on a
resistor we know should be <span class="math inline">\(3.0\
\Omega\)</span>. We measured <span class="math inline">\(3.1\
\Omega\)</span>, so we need to know how likely it is for a measurement
to deviate by <span class="math inline">\(0.1\ \Omega\)</span> or more.
If the probability is very low, we can safely say the resistor is
manufactured incorrectly. Let’s say we got a standard deviation of <span
class="math inline">\(0.2\ \Omega\)</span>. In this case, the
probability of the deviation is 62 %, so it is plausible the resistor is
manufactured correctly. If, on the other hand, we get a standard
deviation of <span class="math inline">\(0.04\ \Omega\)</span>, the
probability of the deviation is only 1.2 %, and the resistor is likely
manufactured incorrectly.</p>
<h4 id="assumptions">Assumptions</h4>
<p>To begin estimating our uncertainties, we must make some assumptions.
Namely, we assume our results <span class="math inline">\(f_i\)</span>
are related to the positions <span class="math inline">\(x_i\)</span> by
the relation <span class="math display">\[
  f_i = f(x_i, \boldsymbol{\theta}^*) + e_i,
\]</span> where <span
class="math inline">\(\boldsymbol{\theta}^*\)</span> are the optimal
parameters and <span class="math inline">\(e_i\)</span> are some unknown
errors resulting from, for instance, imprecise measurements. Further, we
assume that the <span class="math inline">\(e_i\)</span>-values are
independent, meaning they don’t depend on each other, and that they
follow a normal distribution with a mean of zero and an unknown variance
<span class="math inline">\(\sigma^2\)</span>. This is not always true,
but the central limit theorem guarantees that the errors will be
approximately normally distributed if we have a large amount of data
points.</p>
<h4 id="covariance">Covariance</h4>
<p>To get an estimate of the uncertainty of our optimal parameters, we
need to calculate the so-called covariance matrix, which has the
variance of the parameters as it’s diagonal, and the covariance between
parameters as the other elements. We are only interested in the variance
of the parameters, as this is the square of their standard deviations.
As we are not interested in the covariance between the parameters, I
will not explain what that is.</p>
<p>From the paper <em>Nonlinear Regression</em> by A. R. Gallant, we get
that the covariance matrix is given by <span class="math display">\[
  V =
\sigma^2\left[F^T(\boldsymbol{\theta}^*)F(\boldsymbol{\theta}^*)\right]^{-1},
\]</span> where <span class="math display">\[
  F(\boldsymbol{\theta}) = \begin{bmatrix}
    \big(\nabla f(x_1; \boldsymbol{\theta})\big)^T \\
    \vdots \\
    \big(\nabla f(x_N; \boldsymbol{\theta})\big)^T
  \end{bmatrix}.
\]</span></p>
<p>We don’t know <span class="math inline">\(\sigma^2\)</span> or <span
class="math inline">\(\boldsymbol{\theta}^*\)</span>, but we have our
calculated optimal parameters <span
class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, and we can
estimate the variance of <span class="math inline">\(e_i\)</span> using
the formula <span class="math display">\[
  \sigma^2 \approx s^2 = \frac{1}{N-n}\sum\limits_{i=1}^N (f_i - f(x_i;\
\hat{\boldsymbol{\theta}}))^2.
\]</span> This gives the estimated covariance matrix <span
class="math display">\[
\begin{align*}
  \hat{V} &amp;=
s^2\left[F^T(\hat{\boldsymbol{\theta}})F(\hat{\boldsymbol{\theta}})\right]^{-1}
\\
  &amp;= s^2\left[\sum\limits_{i=1}^N\nabla f(x_i;
\hat{\boldsymbol{\theta}})\big(\nabla f(x_i;
\hat{\boldsymbol{\theta}})\big)^T\right]^{-1},
\end{align*}
\]</span> which has the diagonal elements <span
class="math inline">\(\left((\Delta \theta_1)^2, \ldots, (\Delta
\theta_n)^2\right).\)</span> We can then present our calculated optimal
parameters as <span
class="math inline">\((\hat{\theta}_1\pm\Delta\theta_1, \ldots,
\hat{\theta}_n\pm\Delta\theta_n).\)</span></p>
<h2 id="omega-optimizer">Omega Optimizer</h2>
<h3 id="normal-distribution">Normal distribution</h3>
<p>To test <em>Omega Optimizer</em>, we start with a simple case: a
normal distribution. We have the data:</p>
<p><img src="media/normal_data.png" /></p>
<p>To which we want to fit <span class="math inline">\(f(x; a, \mu,
\sigma) = ae^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\)</span>. Here we
immediately reach a problem with <em>Omega Optimizer</em>; you must
calculate the gradient and hessian of <span
class="math inline">\(f\)</span> manually. Ideally, this would be done
for you, but I have not found a good automatic differentiation tool for
rust yet. However, after calculating the derivatives, <em>Omega
Optimizer</em> can find the optimal parameters with no help. Even
starting with all parameters equal to 1, Newton’s method quickly
converges to the global minimum, giving</p>
<p><img src="media/normal_fit.png" /></p>
<p>which has the parameters</p>
<blockquote>
<p>(<span class="math inline">\(a\)</span>, <span
class="math inline">\(\mu\)</span>, <span
class="math inline">\(\sigma\)</span>) = (<span
class="math inline">\(5.60\pm 0.06\)</span>, <span
class="math inline">\(2.36\pm 0.07\)</span>, <span
class="math inline">\(6.05\pm 0.07\)</span>)</p>
</blockquote>
<p>and a mean squared error of <span
class="math inline">\(0.0618\)</span>.</p>
<h3 id="sine-wave">Sine wave</h3>
<p>We now want to test a slightly more complicated example, namely a
sine wave. We have the data:</p>
<p><img src="media/sine_data.png" /></p>
<p>To which we want to fit <span class="math inline">\(f(x; \omega,
\phi, a, b) = a\sin(\omega x + \phi) + b\)</span>. If we again run
<em>Omega Optimizer</em> with all parameters equal to 1, we get</p>
<p><img src="media/sine_line.png" /></p>
<p>which has the parameters</p>
<blockquote>
<p>(<span class="math inline">\(\omega\)</span>, <span
class="math inline">\(\phi\)</span>, <span
class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>) = (0.13, 2.7, 20, 22)</p>
</blockquote>
<p>and a mean squared error of 305. This is obviously not the correct
solution, but it is actually the optimal line fit to the data. A
straight line is a typical example of a local minimum, so this is not an
unexpected result. To get the global minimum, we need better initial
parameters. To do this, I have integrated a gui into <em>Omega
Optimizer</em> where you can change the parameters to get a good
starting point:</p>
<p><img src="media/gui.png" /></p>
<p>With this, the program converges to the true solution</p>
<p><img src="media/sine_fit.png" /></p>
<p>which has the parameters</p>
<blockquote>
<p>(<span class="math inline">\(\omega\)</span>, <span
class="math inline">\(\phi\)</span>, <span
class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>) = (<span class="math inline">\(9.82\pm
0.07\)</span>, <span class="math inline">\(0.97\pm 0.05\)</span>, <span
class="math inline">\(23\pm 1\)</span>, <span
class="math inline">\(29.7\pm 0.9\)</span>)</p>
</blockquote>
<p>and an error of 77.4.</p>
<p>For this example, it is only the frequency which made convergence
difficult. Even starting at <span class="math inline">\((10, 1, 1,
1)\)</span>, the program converges to the correct solution. This makes
sense, the local line minimum is much closer than the true minimum in
<span class="math inline">\(\omega\)</span>-space, so that is what the
program naturally converges to.</p>
<h3 id="mortivated">Mortivated</h3>
<p>It is now time to answer the question my brother asked me once and
for all. We have the data:</p>
<p><img src="media/mort_data.png" /></p>
<p>To which we want to fit <span class="math inline">\(f(x; a, b, c, n)
= \frac{ax^n}{bx^n + 1} + c\)</span>. For this function, <em>Omega
Optimizer</em> can once again find the global minimum starting with all
parameters equal to 1, giving</p>
<p><img src="media/mort_fit.png" /></p>
<p>which has the parameters</p>
<blockquote>
<p>(<span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>, <span
class="math inline">\(c\)</span>, <span
class="math inline">\(n\)</span>) = (<span class="math inline">\(0.29\pm
0.03\)</span>, <span class="math inline">\(0.00033\pm 0.00003\)</span>,
<span class="math inline">\(80\pm 4\)</span>, <span
class="math inline">\(2.07\pm 0.03\)</span>)</p>
</blockquote>
<p>and an error of 141. The parameters I found manually way back when
was</p>
<blockquote>
<p>(a, b, c, n) = (0.09, 0.000106, 98, 2.32)</p>
</blockquote>
<p>which has an error of 343. This means <em>Omega Optimizer</em> not
only found the minimum with no manual input, the parameters it found are
also more than twice as good as those I found manually.</p>
<p>Looking at the best fit curve, it is obvious it does not actually fit
the data perfectly. I don’t think this is a problem with the parameters,
I think that the data does not actually follow the curve I hypothesized.
There are other functions which have a similar shape, such as the
logistic function or the error function. However, the function I have
found is a good enough approximation of the true function that I don’t
think it is necessary to use more time on this particular problem.</p>
<h3 id="conclusion-and-further-developments">Conclusion and further
developments</h3>
<p>From the examples I have shown, it is clear that <em>Omega
Optimizer</em> works well. It is written entirely in rust, so it is very
fast; for the normal distribution example, it used just <span
class="math inline">\(0.9\)</span> ms to converge to the solution. It is
a bit slower when it does not reach a good minimum since I try <span
class="math inline">\(10^4\)</span> gradient descent steps before giving
up, with the sine example taking <span class="math inline">\(66\)</span>
ms when trying to converge to a straight line.</p>
<p>However, there are two downsides to the program, which are the two
manual parts; calculating the gradient and hessian, and finding good
initial parameters. The first problem could be fixed using automatic
differentiation, but as the calculation only has to be done once per
function, I don’t think it is that big of a problem. The other problem
has no good solution that I know of. The program is fast enough that I
could try many sets of initial parameters, but as we don’t have a
bounded search space, I don’t know how this could be done. The only
approach I think might work is a sort of deflation method, but I have
not gotten it to work. At the end of the day, using the gui is not that
difficult and it is often not necessary, so I think it is a fine
solution for now.</p>
<p>If you want to look at the source code for <em>Omega Optimizer</em>,
it is available at <a
href="https://github.com/Emilinya/OmegaOptimizer">my github</a>.</p>
<h2 id="how-it-is-actually-done">How it is actually done</h2>
<p>As I mentioned in the introduction, non-linear parameter optimization
is typically done with an approach that depends on the specific problem.
Here, I want to show how this would be used to find the parameters for
the function <span class="math display">\[f(x; a, b, c, n) =
\frac{ax^n}{bx^n + 1} + c.\]</span></p>
<p>The general idea is to go from <span class="math inline">\(y =
f(x)\)</span> to <span class="math inline">\(y&#39; = \alpha x&#39; +
\beta\)</span>, where <span class="math inline">\(y&#39;\)</span> and
<span class="math inline">\(x&#39;\)</span> are some transformations of
<span class="math inline">\(y\)</span> and <span
class="math inline">\(x\)</span>. Once the data follows a linear
relationship, you can use linear regression to find <span
class="math inline">\(\alpha\)</span> and <span
class="math inline">\(\beta\)</span>, which you can then use to recover
your parameters. For example, for data that follows some power law <span
class="math display">\[f(x; a, n) = ax^n,\]</span> we could take the
logarithm of both sides, giving us <span class="math display">\[
\begin{align*}
    \ln(y) &amp;= \ln(ax^n) \\
    &amp;= \ln(a) + n\ln(x)
\end{align*}
\]</span> That is, if we set <span class="math inline">\(y&#39; =
\ln(y)\)</span> and <span class="math inline">\(x&#39; =
\ln(x)\)</span>, we can use linear regression to find <span
class="math inline">\(\ln(a)\)</span> and <span
class="math inline">\(n\)</span>.</p>
<p>The problem is that this approach only really works if you have two
parameters, and we have four. Luckily, we can use some known facts about
the function to reduce the number of parameters. The most obvious one is
<span class="math inline">\(c\)</span>, which is the value where the
function crosses the x-axis. From the data points we get that <span
class="math inline">\(c\approx 92\)</span>. Another thing we can tell is
that, as x becomes bigger, <span class="math inline">\(\frac{ax^n}{bx^n
+ 1}\)</span> approaches the constant <span
class="math inline">\(\frac{a}{b}\)</span>. If we define <span
class="math display">\[\lim_{x\rightarrow\infty} f(x; a, b, c, n) =
\frac{a}{b} + c := \bar{y},\]</span> we find <span
class="math inline">\(b = \frac{a}{\bar{y} - c}\)</span>. We don’t know
the exact value of <span class="math inline">\(\bar{y}\)</span>, but
from the data we know it is a bit more than <span
class="math inline">\(929\)</span>. We have now reduced the number of
free parameters to 2, so the next step is to find <span
class="math inline">\(y&#39;\)</span> and <span
class="math inline">\(x&#39;\)</span>: <span class="math display">\[
\begin{align*}
    y &amp;= \frac{ax^n}{\frac{ax^n}{\bar{y} - c} + 1} + c \\
    &amp;= \frac{1}{\frac{1}{\bar{y} - c} + \frac{1}{ax^n}} + c \\
    \implies \frac{1}{y - c} &amp;= \frac{1}{\bar{y} - c} +
\frac{1}{ax^n} \\
    \implies \frac{1}{y - c} - \frac{1}{\bar{y} - c} &amp;=
\frac{1}{ax^n} \\
    \implies \ln\left(\frac{1}{y - c} - \frac{1}{\bar{y} - c}\right)
&amp;= \ln\left(\frac{1}{ax^n}\right) \\
    &amp;= -\ln(a) - n\ln(x)
\end{align*}
\]</span> So, if we define <span class="math inline">\(y&#39; =
-\ln\left(\frac{1}{y - c} - \frac{1}{\bar{y} - c}\right)\)</span> and
<span class="math inline">\(x&#39; = \ln(x)\)</span>, we get <span
class="math inline">\(y&#39; = \ln(a) + nx&#39;\)</span>, just like in
the power law example.</p>
<p>If we use <span class="math inline">\(c=92\)</span>, <span
class="math inline">\(\bar{y}=930\)</span> and find <span
class="math inline">\(a\)</span> and <span
class="math inline">\(n\)</span> using linear regression, we get</p>
<p><img src="media/y_930.png" /></p>
<p>which has the parameters</p>
<blockquote>
<p>(a, b, c, n) = (0.0973, 0.000116, 92, 2.37)</p>
</blockquote>
<p>and an error of 422. Certainly not bad, but worse than what <em>Omega
Optimizer</em> got. To get a better result, we must use a better value
for <span class="math inline">\(\bar{y}\)</span>. This again requires
the use of some optimization method, but the problem is now
one-dimensional, and we now have a bounded search space, as we know that
the solution should be between <span class="math inline">\(930\)</span>
and <span class="math inline">\(1000\)</span>. This makes the
optimization much simpler. Doing this, we get <span
class="math inline">\(\bar{y} = 946\)</span>, giving us</p>
<p><img src="media/y_946.png" /></p>
<p>which has the parameters</p>
<blockquote>
<p>(a, b, c, n) = (0.273, 0.000320, 92, 2.07)</p>
</blockquote>
<p>and an error of 153. This is very close, but still a bit worse than
the result from <em>Omega Optimizer</em>. I also tried varying the value
of <span class="math inline">\(c\)</span>, but I could not get a
meaningfully better result.</p>
<p>This shows that the ‘standard’ approach works very well, as expected,
but it requires a lot more manual calculation than <em>Omega
Optimizer</em>. However, this argument would be more convincing if
<em>Omega Optimizer</em> did not require manual calculation of the
derivatives and manual selection of good initial parameters. For this
reason, I can’t really say which approach is better. One takeaway from
this is probably that, to make <em>Omega Optimizer</em> easier to use,
you should probably parameterize your functions with parameters you know
what represent, like using <span class="math inline">\(\bar{y}\)</span>
instead of <span class="math inline">\(b\)</span>, as this makes finding
decent parameters with the gui much easier.</p>
</body>
</html>
